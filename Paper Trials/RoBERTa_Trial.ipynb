{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvu0T09KsRZC",
        "outputId": "915d686b-4884-4db9-9835-6a04ed4e5292"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW\n",
        "\n",
        "def fine_tune_roberta_for_rumor_detection(train_texts, train_labels, val_texts, val_labels, epochs=3, batch_size=8, learning_rate=2e-5):\n",
        "    # Initialize RoBERTa tokenizer and model\n",
        "    tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "    model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2)\n",
        "\n",
        "    model.to('cuda')\n",
        "\n",
        "    # Tokenize and encode the training and validation sets\n",
        "    train_encodings = tokenizer(train_texts, truncation=True, padding=True, return_tensors='pt')\n",
        "    val_encodings = tokenizer(val_texts, truncation=True, padding=True, return_tensors='pt')\n",
        "\n",
        "    # Move tensors to GPU\n",
        "    train_encodings = {key: value.to('cuda') for key, value in train_encodings.items()}\n",
        "    val_encodings = {key: value.to('cuda') for key, value in val_encodings.items()}\n",
        "\n",
        "    # Convert labels to PyTorch tensors\n",
        "    train_labels = torch.tensor(train_labels).to('cuda')\n",
        "    val_labels = torch.tensor(val_labels).to('cuda')\n",
        "\n",
        "    # Create PyTorch datasets\n",
        "    train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], train_labels)\n",
        "    val_dataset = TensorDataset(val_encodings['input_ids'], val_encodings['attention_mask'], val_labels)\n",
        "\n",
        "    # Create PyTorch data loaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Set up optimizer and scheduler\n",
        "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "    total_steps = len(train_loader) * epochs\n",
        "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=learning_rate, total_steps=total_steps)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for batch in train_loader:\n",
        "            input_ids, attention_mask, labels = batch\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        correct_predictions = 0\n",
        "        total_samples = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                input_ids, attention_mask, labels = batch\n",
        "                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "                val_loss += outputs.loss.item()\n",
        "                logits = outputs.logits\n",
        "                predictions = torch.argmax(logits, dim=1)\n",
        "                correct_predictions += (predictions == labels).sum().item()\n",
        "                total_samples += labels.size(0)\n",
        "\n",
        "        average_val_loss = val_loss / len(val_loader)\n",
        "        accuracy = correct_predictions / total_samples\n",
        "\n",
        "        print(f'Epoch {epoch + 1}/{epochs}, Val Loss: {average_val_loss}, Accuracy: {accuracy}')\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "ZuXp4h9953k8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "JmWVqstj832x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d bjoernjostein/fake-news-data-set"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-nn1P_s77bI5",
        "outputId": "af5caa10-5552-4658-cac0-9727ff61b7bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading fake-news-data-set.zip to /content\n",
            " 76% 41.0M/53.8M [00:00<00:00, 49.8MB/s]\n",
            "100% 53.8M/53.8M [00:00<00:00, 56.8MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip fake-news-data-set.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4fB_wYc8_65",
        "outputId": "4bb1469d-ddee-4744-d4a5-80ed2fb22b74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  fake-news-data-set.zip\n",
            "  inflating: sample_submission.csv   \n",
            "  inflating: test/test.csv           \n",
            "  inflating: train/train.csv         \n",
            "  inflating: val/val.csv             \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path = '/content/train/train.csv'\n",
        "train_df = pd.read_csv(file_path)\n",
        "print(train_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "APXQk32-9UFj",
        "outputId": "c322ce02-fa6d-42b5-ffad-9f937973e2b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text label\n",
            "0  The court granted by a 5-4 vote a request made...  real\n",
            "1  \" Pennsylvania was a crucial swing state in th...  real\n",
            "2  The company today is rolling out an update to ...  fake\n",
            "3  When it comes to trade policy, Hillary Clinton...  real\n",
            "4  S. stocks had their worst April start since 19...  real\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/val/val.csv'\n",
        "val_df = pd.read_csv(file_path)\n",
        "print(val_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFwISvU-AHDX",
        "outputId": "4902fa7a-1bfa-426f-9627-e82d0e51d12f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text label\n",
            "0  Police investigating Saturday's fatal shooting...  real\n",
            "1  A car bomb in Turkey's eastern province of Ela...  real\n",
            "2  Democratic presidential candidate Hillary Clin...  real\n",
            "3  Generally, each party gets two turns to decide...  fake\n",
            "4  Securitas CEO Alf Goransson told Reuters: * Se...  real\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_text = train_df.iloc[:, 0].tolist()\n",
        "train_labels = train_df.iloc[:, 1].tolist()\n",
        "val_text = val_df.iloc[:, 0].tolist()\n",
        "val_labels = val_df.iloc[:, 1].tolist()"
      ],
      "metadata": {
        "id": "oApJ7X5T98-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, label in enumerate(train_labels):\n",
        "  train_labels[i] = 0 if label == 'real' else 1\n",
        "\n",
        "for i, label in enumerate(val_labels):\n",
        "  val_labels[i] = 0 if label == 'real' else 1"
      ],
      "metadata": {
        "id": "0rtOLQlfD_10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = fine_tune_roberta_for_rumor_detection(train_text[0:1000], train_labels[0:1000], val_text[0:1000], val_labels[0:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrxOr1QgAQ6I",
        "outputId": "ec635970-99b4-4ab4-cb5c-f31c73230ca0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3, Val Loss: 0.06791392429172993, Accuracy: 0.98\n",
            "Epoch 2/3, Val Loss: 0.03762195019610226, Accuracy: 0.988\n",
            "Epoch 3/3, Val Loss: 0.07925761113222689, Accuracy: 0.98\n"
          ]
        }
      ]
    }
  ]
}