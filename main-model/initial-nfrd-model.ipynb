{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch transformers","metadata":{"execution":{"iopub.status.busy":"2024-05-11T23:38:51.446909Z","iopub.execute_input":"2024-05-11T23:38:51.447598Z","iopub.status.idle":"2024-05-11T23:39:04.998003Z","shell.execute_reply.started":"2024-05-11T23:38:51.447567Z","shell.execute_reply":"2024-05-11T23:39:04.996909Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.1.2)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.39.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.2.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.22.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install supabase","metadata":{"execution":{"iopub.status.busy":"2024-05-11T23:39:11.629262Z","iopub.execute_input":"2024-05-11T23:39:11.629642Z","iopub.status.idle":"2024-05-11T23:39:36.171684Z","shell.execute_reply.started":"2024-05-11T23:39:11.629599Z","shell.execute_reply":"2024-05-11T23:39:36.170529Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting supabase\n  Downloading supabase-2.4.5-py3-none-any.whl.metadata (9.3 kB)\nCollecting gotrue<3.0,>=1.3 (from supabase)\n  Downloading gotrue-2.4.2-py3-none-any.whl.metadata (6.1 kB)\nRequirement already satisfied: httpx<0.28,>=0.24 in /opt/conda/lib/python3.10/site-packages (from supabase) (0.27.0)\nCollecting postgrest<0.17.0,>=0.14 (from supabase)\n  Downloading postgrest-0.16.4-py3-none-any.whl.metadata (5.1 kB)\nCollecting realtime<2.0.0,>=1.0.0 (from supabase)\n  Downloading realtime-1.0.4-py3-none-any.whl.metadata (2.6 kB)\nCollecting storage3<0.8.0,>=0.5.3 (from supabase)\n  Downloading storage3-0.7.4-py3-none-any.whl.metadata (1.9 kB)\nCollecting supafunc<0.5.0,>=0.3.1 (from supabase)\n  Downloading supafunc-0.4.5-py3-none-any.whl.metadata (1.2 kB)\nRequirement already satisfied: pydantic<3,>=1.10 in /opt/conda/lib/python3.10/site-packages (from gotrue<3.0,>=1.3->supabase) (2.5.3)\nRequirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx<0.28,>=0.24->supabase) (4.2.0)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx<0.28,>=0.24->supabase) (2024.2.2)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<0.28,>=0.24->supabase) (1.0.5)\nRequirement already satisfied: idna in /opt/conda/lib/python3.10/site-packages (from httpx<0.28,>=0.24->supabase) (3.6)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx<0.28,>=0.24->supabase) (1.3.0)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<0.28,>=0.24->supabase) (0.14.0)\nRequirement already satisfied: deprecation<3.0.0,>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from postgrest<0.17.0,>=0.14->supabase) (2.1.0)\nCollecting strenum<0.5.0,>=0.4.9 (from postgrest<0.17.0,>=0.14->supabase)\n  Downloading StrEnum-0.4.15-py3-none-any.whl.metadata (5.3 kB)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from realtime<2.0.0,>=1.0.0->supabase) (2.9.0.post0)\nCollecting typing-extensions<5.0.0,>=4.11.0 (from realtime<2.0.0,>=1.0.0->supabase)\n  Downloading typing_extensions-4.11.0-py3-none-any.whl.metadata (3.0 kB)\nRequirement already satisfied: websockets<13,>=11 in /opt/conda/lib/python3.10/site-packages (from realtime<2.0.0,>=1.0.0->supabase) (12.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from deprecation<3.0.0,>=2.1.0->postgrest<0.17.0,>=0.14->supabase) (21.3)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.10->gotrue<3.0,>=1.3->supabase) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.10->gotrue<3.0,>=1.3->supabase) (2.14.6)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.8.1->realtime<2.0.0,>=1.0.0->supabase) (1.16.0)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx<0.28,>=0.24->supabase) (1.2.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->deprecation<3.0.0,>=2.1.0->postgrest<0.17.0,>=0.14->supabase) (3.1.1)\nDownloading supabase-2.4.5-py3-none-any.whl (15 kB)\nDownloading gotrue-2.4.2-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading postgrest-0.16.4-py3-none-any.whl (20 kB)\nDownloading realtime-1.0.4-py3-none-any.whl (8.9 kB)\nDownloading storage3-0.7.4-py3-none-any.whl (15 kB)\nDownloading supafunc-0.4.5-py3-none-any.whl (6.1 kB)\nDownloading StrEnum-0.4.15-py3-none-any.whl (8.9 kB)\nDownloading typing_extensions-4.11.0-py3-none-any.whl (34 kB)\nInstalling collected packages: strenum, typing-extensions, realtime, supafunc, storage3, postgrest, gotrue, supabase\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing_extensions 4.9.0\n    Uninstalling typing_extensions-4.9.0:\n      Successfully uninstalled typing_extensions-4.9.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cubinlinker, which is not installed.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires ptxcompiler, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 15.0.2 which is incompatible.\ncudf 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.4.0 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncudf 23.8.0 requires pyarrow==11.*, but you have pyarrow 15.0.2 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.1 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.1 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\njupyterlab 4.1.6 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.2.1 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed gotrue-2.4.2 postgrest-0.16.4 realtime-1.0.4 storage3-0.7.4 strenum-0.4.15 supabase-2.4.5 supafunc-0.4.5 typing-extensions-4.11.0\n","output_type":"stream"}]},{"cell_type":"code","source":"!huggingface-cli login --token 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","metadata":{"execution":{"iopub.status.busy":"2024-05-11T23:39:36.173774Z","iopub.execute_input":"2024-05-11T23:39:36.174138Z","iopub.status.idle":"2024-05-11T23:39:37.731915Z","shell.execute_reply.started":"2024-05-11T23:39:36.174082Z","shell.execute_reply":"2024-05-11T23:39:37.731043Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm \nimport torch\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torch.nn import CrossEntropyLoss\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW, AutoConfig\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\nfrom kaggle_secrets import UserSecretsClient\nfrom supabase import create_client, ClientOptions\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2024-05-11T23:39:37.733403Z","iopub.execute_input":"2024-05-11T23:39:37.733715Z","iopub.status.idle":"2024-05-11T23:39:44.407685Z","shell.execute_reply.started":"2024-05-11T23:39:37.733687Z","shell.execute_reply":"2024-05-11T23:39:44.406847Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def train_model_with_ewc(train_encodings, val_encodings, train_labels, val_labels, tokenizer, model, device, model_save_path):\n    # Initialize EWC\n    ewc = initialize_ewc(model, tokenizer, device)\n    \n    # Convert data to PyTorch tensors and move them to GPU\n    train_encodings = {key: value.to(device) for key, value in train_encodings.items()}\n    val_encodings = {key: value.to(device) for key, value in val_encodings.items()}\n    train_labels = torch.tensor(train_labels).to(device)\n    val_labels = torch.tensor(val_labels).to(device)\n    \n    # Create PyTorch datasets\n    train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], train_labels)\n    val_dataset = TensorDataset(val_encodings['input_ids'], val_encodings['attention_mask'], val_labels)\n    \n    # Create PyTorch data loaders\n    batch_size = 4\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n    \n    # Fine-tune model with EWC\n    new_model = fine_tune_roberta_for_rumor_detection(model, tokenizer, train_loader, val_loader, model_save_path='NFRD/nfrd-model')\n    \n    return new_model","metadata":{"execution":{"iopub.status.busy":"2024-05-11T23:39:44.490335Z","iopub.execute_input":"2024-05-11T23:39:44.490622Z","iopub.status.idle":"2024-05-11T23:39:44.501879Z","shell.execute_reply.started":"2024-05-11T23:39:44.490599Z","shell.execute_reply":"2024-05-11T23:39:44.500964Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"execution":{"iopub.status.busy":"2024-05-11T23:39:44.409609Z","iopub.execute_input":"2024-05-11T23:39:44.410034Z","iopub.status.idle":"2024-05-11T23:39:44.436569Z","shell.execute_reply.started":"2024-05-11T23:39:44.410008Z","shell.execute_reply":"2024-05-11T23:39:44.435243Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class EWC:\n    \n    def __init__(self, prior_model, data_samples, num_sample=30, lambda_=0.1):\n        self.prior_model = prior_model\n        self.prior_weights = [weight.cpu().detach().numpy() for weight in prior_model.parameters()]\n        self.num_sample = num_sample\n        self.data_samples = data_samples\n        self.fisher_matrix = self.compute_fisher()\n        self.lambda_ = lambda_  # Define lambda_ attribute\n        \n    def compute_fisher(self):\n        weights = self.prior_weights\n        fisher_accum = [np.zeros_like(layer) for layer in weights]\n        criterion = torch.nn.CrossEntropyLoss()\n        for j in tqdm(range(self.num_sample)):\n            idx = np.random.randint(self.data_samples.shape[0])\n            input_sample = self.data_samples[idx].unsqueeze(0).to(self.prior_model.device)  # Ensure sample is on the same device as the model\n            outputs = self.prior_model(input_sample)\n            loss = criterion(outputs.logits, torch.tensor([0]).to(outputs.logits.device))  # Assume binary classification\n            gradients = torch.autograd.grad(outputs=loss, inputs=self.prior_model.parameters(), create_graph=True)\n            for m, grad in enumerate(gradients):\n                fisher_accum[m] += grad.detach().cpu().numpy() ** 2  # Move gradient to CPU, detach, and convert to NumPy array\n        fisher_accum = [fisher / self.num_sample for fisher in fisher_accum]\n        return fisher_accum\n    \n    def compute_penalty_loss(self, model):\n        penalty = 0.\n        for fisher, param, param_prior in zip(self.fisher_matrix, model.parameters(), self.prior_model.parameters()):\n            param_numpy = param.detach().cpu().numpy()  # Convert tensor to NumPy array\n            param_prior_numpy = param_prior.detach().cpu().numpy()  # Convert tensor to NumPy array\n            penalty += torch.sum(torch.tensor(fisher) * ((param_numpy - param_prior_numpy) ** 2))  # Convert NumPy array to tensor\n        return 0.5 * self.lambda_ * penalty\n    \n    def get_fisher(self):\n        return self.fisher_matrix","metadata":{"execution":{"iopub.status.busy":"2024-05-11T23:39:44.438230Z","iopub.execute_input":"2024-05-11T23:39:44.438548Z","iopub.status.idle":"2024-05-11T23:39:44.455300Z","shell.execute_reply.started":"2024-05-11T23:39:44.438496Z","shell.execute_reply":"2024-05-11T23:39:44.454306Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def fine_tune_roberta_for_rumor_detection(model, tokenizer, train_loader, val_loader, ewc=None, epochs=20, learning_rate=1e-5, patience=5, model_save_path='NFRD/nfrd-model'):\n    # Set up optimizer\n    optimizer = AdamW(model.parameters(), lr=learning_rate)\n\n    # Initialize early stopping variables\n    best_val_loss = float('inf')    \n    no_improvement_counter = 0\n\n    # Training loop\n    for epoch in range(epochs):\n        model.train()\n        train_correct_predictions = 0\n        train_total_samples = 0\n\n        for batch in train_loader:\n            input_ids, attention_mask, labels = batch\n            optimizer.zero_grad()\n            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n            loss = outputs.loss\n            \n            # Calculate additional EWC loss\n            if ewc is not None:\n                ewc_loss = ewc.compute_penalty_loss(model)\n                loss += ewc_loss\n            \n            loss.backward()\n            optimizer.step()\n\n            # Calculate training accuracy\n            logits = outputs.logits\n            predictions = torch.argmax(logits, dim=1)\n            train_correct_predictions += (predictions == labels).sum().item()\n            train_total_samples += labels.size(0)\n\n        # Validation\n        model.eval()\n        val_loss = 0.0\n        correct_predictions = 0\n        total_samples = 0\n\n        with torch.no_grad():\n            for batch in val_loader:\n                input_ids, attention_mask, labels = batch\n                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n                val_loss += outputs.loss.item()\n                logits = outputs.logits\n                predictions = torch.argmax(logits, dim=1)\n                correct_predictions += (predictions == labels).sum().item()\n                total_samples += labels.size(0)\n\n        average_val_loss = val_loss / len(val_loader)\n        accuracy = correct_predictions / total_samples\n        train_accuracy = train_correct_predictions / train_total_samples\n\n        print(f'Epoch {epoch + 1}/{epochs}, '\n              f'Train Loss: {loss.item()}, Train Accuracy: {train_accuracy:.4f}, '\n              f'Val Loss: {average_val_loss:.4f}, Val Accuracy: {accuracy:.4f}')\n\n        # Check for early stopping\n        if average_val_loss < best_val_loss:\n            best_val_loss = average_val_loss\n            no_improvement_counter = 0\n            \n            # Save the model\n            model.push_to_hub(model_save_path)\n            tokenizer.push_to_hub(model_save_path)\n            print(f'Model checkpoint saved to {model_save_path}')\n            print(f'Model uploaded to the Hugging Face Model Hub')\n        else:\n            no_improvement_counter += 1\n\n        if no_improvement_counter >= patience:\n            print(f'Early stopping after {epoch + 1} epochs without improvement.')\n            break\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-05-11T23:39:44.456734Z","iopub.execute_input":"2024-05-11T23:39:44.457011Z","iopub.status.idle":"2024-05-11T23:39:44.473070Z","shell.execute_reply.started":"2024-05-11T23:39:44.456987Z","shell.execute_reply":"2024-05-11T23:39:44.472176Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def initialize_ewc(model, tokenizer, device):\n    ewc_text = \"example text\"  # You need to provide example text here\n    ewc_encodings = tokenizer(ewc_text, truncation=True, padding=True, return_tensors='pt')\n    ewc_encodings = {key: value.to(device) for key, value in ewc_encodings.items()}\n    data_samples = ewc_encodings['input_ids']  # Sample a small subset of data\n    ewc = EWC(prior_model=model, data_samples=data_samples, num_sample=300, lambda_=10)\n    return ewc","metadata":{"execution":{"iopub.status.busy":"2024-05-11T23:39:44.502908Z","iopub.execute_input":"2024-05-11T23:39:44.503164Z","iopub.status.idle":"2024-05-11T23:39:44.513338Z","shell.execute_reply.started":"2024-05-11T23:39:44.503143Z","shell.execute_reply":"2024-05-11T23:39:44.512540Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def fetch_data_from_supabase(schema, table, field, num_of_records):\n    user_secrets = UserSecretsClient()\n\n    # Connect to supabase\n    supabase_url = \"https://fglqovplibiyttjzqxuj.supabase.co\"\n    supabase_key = user_secrets.get_secret(\"SUPABASE_KEY\")\n\n    supabase = create_client(\n                supabase_url,\n                supabase_key,\n                options=ClientOptions(\n                  schema=schema\n                ))\n\n\n    resp = supabase.table(table).select(field).limit(num_of_records).execute()\n    data = resp.data\n\n    data = [row[field] for row in data]\n    return data","metadata":{"execution":{"iopub.status.busy":"2024-05-11T23:39:44.474292Z","iopub.execute_input":"2024-05-11T23:39:44.474767Z","iopub.status.idle":"2024-05-11T23:39:44.486801Z","shell.execute_reply.started":"2024-05-11T23:39:44.474732Z","shell.execute_reply":"2024-05-11T23:39:44.485939Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## Initial Training","metadata":{}},{"cell_type":"code","source":"true_news = fetch_data_from_supabase(\"text_datasets\", \"true_text_dataset\", \"text\", 85000)\nfalse_news = fetch_data_from_supabase(\"text_datasets\", \"false_text_dataset\", \"text\", 85000)","metadata":{"execution":{"iopub.status.busy":"2024-05-11T23:43:45.736283Z","iopub.execute_input":"2024-05-11T23:43:45.737096Z","iopub.status.idle":"2024-05-11T23:43:59.287255Z","shell.execute_reply.started":"2024-05-11T23:43:45.737059Z","shell.execute_reply":"2024-05-11T23:43:59.286215Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Add labels and prepare data\nall_texts = true_news + false_news\nall_labels = [0] * len(true_news) + [1] * len(false_news)\ntrue_news = None\nfalse_news = None","metadata":{"execution":{"iopub.status.busy":"2024-05-10T11:30:29.779902Z","iopub.execute_input":"2024-05-10T11:30:29.780184Z","iopub.status.idle":"2024-05-10T11:30:29.785113Z","shell.execute_reply.started":"2024-05-10T11:30:29.780161Z","shell.execute_reply":"2024-05-10T11:30:29.784099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_texts, val_texts, train_labels, val_labels = train_test_split(all_texts, all_labels, test_size=0.2, random_state=42)\nall_texts = None\nall_labels = None","metadata":{"execution":{"iopub.status.busy":"2024-05-10T11:30:29.786103Z","iopub.execute_input":"2024-05-10T11:30:29.786339Z","iopub.status.idle":"2024-05-10T11:30:29.809458Z","shell.execute_reply.started":"2024-05-10T11:30:29.786319Z","shell.execute_reply":"2024-05-10T11:30:29.808678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained('roberta-base')\nmodel = AutoModelForSequenceClassification.from_pretrained('roberta-base', num_labels=2)","metadata":{"execution":{"iopub.status.busy":"2024-05-10T13:37:54.229985Z","iopub.execute_input":"2024-05-10T13:37:54.230514Z","iopub.status.idle":"2024-05-10T13:38:01.653820Z","shell.execute_reply.started":"2024-05-10T13:37:54.230478Z","shell.execute_reply":"2024-05-10T13:38:01.651532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-05-10T11:30:39.242147Z","iopub.execute_input":"2024-05-10T11:30:39.242613Z","iopub.status.idle":"2024-05-10T11:30:39.528131Z","shell.execute_reply.started":"2024-05-10T11:30:39.242585Z","shell.execute_reply":"2024-05-10T11:30:39.527188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Tokenize and encode the training and validation sets\ntrain_encodings = tokenizer(train_texts, truncation=True, padding=True, return_tensors='pt')\nval_encodings = tokenizer(val_texts, truncation=True, padding=True, return_tensors='pt')","metadata":{"execution":{"iopub.status.busy":"2024-05-10T11:30:39.546235Z","iopub.execute_input":"2024-05-10T11:30:39.546526Z","iopub.status.idle":"2024-05-10T11:30:48.673442Z","shell.execute_reply.started":"2024-05-10T11:30:39.546493Z","shell.execute_reply":"2024-05-10T11:30:48.672389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Move tensors to GPU\ntrain_encodings = {key: value.to(device) for key, value in train_encodings.items()}\nval_encodings = {key: value.to(device) for key, value in val_encodings.items()}","metadata":{"execution":{"iopub.status.busy":"2024-05-10T11:30:48.674594Z","iopub.execute_input":"2024-05-10T11:30:48.674891Z","iopub.status.idle":"2024-05-10T11:30:49.078071Z","shell.execute_reply.started":"2024-05-10T11:30:48.674864Z","shell.execute_reply":"2024-05-10T11:30:49.076996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert labels to PyTorch tensors\ntrain_labels = torch.tensor(train_labels).to(device)\nval_labels = torch.tensor(val_labels).to(device)","metadata":{"execution":{"iopub.status.busy":"2024-05-10T11:30:49.079365Z","iopub.execute_input":"2024-05-10T11:30:49.079672Z","iopub.status.idle":"2024-05-10T11:30:49.090935Z","shell.execute_reply.started":"2024-05-10T11:30:49.079647Z","shell.execute_reply":"2024-05-10T11:30:49.089915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create PyTorch datasets\ntrain_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], train_labels)\nval_dataset = TensorDataset(val_encodings['input_ids'], val_encodings['attention_mask'], val_labels)","metadata":{"execution":{"iopub.status.busy":"2024-05-10T11:30:49.092233Z","iopub.execute_input":"2024-05-10T11:30:49.092511Z","iopub.status.idle":"2024-05-10T11:30:49.102502Z","shell.execute_reply.started":"2024-05-10T11:30:49.092489Z","shell.execute_reply":"2024-05-10T11:30:49.101597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create PyTorch data loaders\nbatch_size = 4\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-05-10T11:30:49.103627Z","iopub.execute_input":"2024-05-10T11:30:49.103890Z","iopub.status.idle":"2024-05-10T11:30:49.112743Z","shell.execute_reply.started":"2024-05-10T11:30:49.103868Z","shell.execute_reply":"2024-05-10T11:30:49.112009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_model = fine_tune_roberta_for_rumor_detection(model, tokenizer, train_loader, val_loader, model_save_path='NFRD/nfrd-model')","metadata":{"execution":{"iopub.status.busy":"2024-05-10T11:30:49.113759Z","iopub.execute_input":"2024-05-10T11:30:49.114106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## EWC Training","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained('NFRD/nfrd-model')\nmodel = AutoModelForSequenceClassification.from_pretrained('NFRD/nfrd-model', num_labels=2)","metadata":{"execution":{"iopub.status.busy":"2024-05-11T23:40:27.057084Z","iopub.execute_input":"2024-05-11T23:40:27.057481Z","iopub.status.idle":"2024-05-11T23:40:38.415009Z","shell.execute_reply.started":"2024-05-11T23:40:27.057451Z","shell.execute_reply":"2024-05-11T23:40:38.413971Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.22k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56c684b0b59e4bf5bb031e1c07963132"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9813ebd61a7640a58cdbb36dd157cfba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07fddd6ad4e741f994a5dc09acf3a95d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0220f286ca284615a5ee3fbb88686430"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/280 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6df7ec703454708a5774081699dcd10"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/735 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d80868dcdd64936b010cb05c62ff2c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20284d0e8d77483798b23ea0ecd710bf"}},"metadata":{}}]},{"cell_type":"code","source":"model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-05-11T23:40:42.368935Z","iopub.execute_input":"2024-05-11T23:40:42.369514Z","iopub.status.idle":"2024-05-11T23:40:42.662441Z","shell.execute_reply.started":"2024-05-11T23:40:42.369479Z","shell.execute_reply":"2024-05-11T23:40:42.661528Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"RobertaForSequenceClassification(\n  (roberta): RobertaModel(\n    (embeddings): RobertaEmbeddings(\n      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n      (position_embeddings): Embedding(514, 768, padding_idx=1)\n      (token_type_embeddings): Embedding(1, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): RobertaEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (classifier): RobertaClassificationHead(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"true_news = fetch_data_from_supabase(\"text_datasets\", \"true_text_dataset\", \"text\", 86000)\nfalse_news = fetch_data_from_supabase(\"text_datasets\", \"false_text_dataset\", \"text\", 86000)","metadata":{"execution":{"iopub.status.busy":"2024-05-11T23:44:05.551687Z","iopub.execute_input":"2024-05-11T23:44:05.552061Z","iopub.status.idle":"2024-05-11T23:44:21.134924Z","shell.execute_reply.started":"2024-05-11T23:44:05.552030Z","shell.execute_reply":"2024-05-11T23:44:21.134107Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# Add labels and prepare data\nall_texts = true_news + false_news\nall_labels = [0] * len(true_news) + [1] * len(false_news)\ntrue_news = None\nfalse_news = None","metadata":{"execution":{"iopub.status.busy":"2024-05-11T23:44:35.030027Z","iopub.execute_input":"2024-05-11T23:44:35.030394Z","iopub.status.idle":"2024-05-11T23:44:35.041907Z","shell.execute_reply.started":"2024-05-11T23:44:35.030365Z","shell.execute_reply":"2024-05-11T23:44:35.040939Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"train_texts, val_texts, train_labels, val_labels = train_test_split(all_texts, all_labels, test_size=0.2, random_state=42)\nall_texts = None\nall_labels = None","metadata":{"execution":{"iopub.status.busy":"2024-05-11T23:44:36.045942Z","iopub.execute_input":"2024-05-11T23:44:36.046325Z","iopub.status.idle":"2024-05-11T23:44:36.160301Z","shell.execute_reply.started":"2024-05-11T23:44:36.046297Z","shell.execute_reply":"2024-05-11T23:44:36.159516Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# Tokenize and encode the ewc data sample\newc_encodings = tokenizer(train_texts[:300], truncation=True, padding=True, return_tensors='pt')\newc_encodings = {key: value.to(device) for key, value in ewc_encodings.items()}","metadata":{"execution":{"iopub.status.busy":"2024-05-11T23:44:37.035917Z","iopub.execute_input":"2024-05-11T23:44:37.036257Z","iopub.status.idle":"2024-05-11T23:44:37.237885Z","shell.execute_reply.started":"2024-05-11T23:44:37.036234Z","shell.execute_reply":"2024-05-11T23:44:37.237103Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# Tokenize and encode the ewc data sample\newc_encodings = tokenizer(val_texts[:300], truncation=True, padding=True, return_tensors='pt')\newc_encodings = {key: value.to(device) for key, value in ewc_encodings.items()}","metadata":{"execution":{"iopub.status.busy":"2024-05-11T23:44:39.068497Z","iopub.execute_input":"2024-05-11T23:44:39.068895Z","iopub.status.idle":"2024-05-11T23:44:39.266083Z","shell.execute_reply.started":"2024-05-11T23:44:39.068864Z","shell.execute_reply":"2024-05-11T23:44:39.265095Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# Initialize EWC\ndata_samples = ewc_encodings['input_ids']  # Sample a small subset of data\newc = EWC(prior_model=model, data_samples=data_samples, num_sample=300, lambda_=10)","metadata":{"execution":{"iopub.status.busy":"2024-05-11T23:44:41.379853Z","iopub.execute_input":"2024-05-11T23:44:41.380238Z","iopub.status.idle":"2024-05-11T23:46:52.894771Z","shell.execute_reply.started":"2024-05-11T23:44:41.380208Z","shell.execute_reply":"2024-05-11T23:46:52.893932Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"  0%|          | 0/300 [00:00<?, ?it/s]We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n100%|██████████| 300/300 [02:10<00:00,  2.29it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"true_news = fetch_data_from_supabase(\"text_datasets\", \"true_text_dataset\", \"text\", 86000)[-1000:]\nfalse_news = fetch_data_from_supabase(\"text_datasets\", \"false_text_dataset\", \"text\", 86000)[-1000:]","metadata":{"execution":{"iopub.status.busy":"2024-05-11T23:48:15.188416Z","iopub.execute_input":"2024-05-11T23:48:15.188849Z","iopub.status.idle":"2024-05-11T23:48:28.869431Z","shell.execute_reply.started":"2024-05-11T23:48:15.188818Z","shell.execute_reply":"2024-05-11T23:48:28.868359Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# Add labels and prepare data\nall_texts = true_news + false_news\nall_labels = [0] * len(true_news) + [1] * len(false_news)\ntrue_news = None\nfalse_news = None","metadata":{"execution":{"iopub.status.busy":"2024-05-11T23:48:28.871159Z","iopub.execute_input":"2024-05-11T23:48:28.871432Z","iopub.status.idle":"2024-05-11T23:48:28.876492Z","shell.execute_reply.started":"2024-05-11T23:48:28.871411Z","shell.execute_reply":"2024-05-11T23:48:28.875372Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"train_texts, val_texts, train_labels, val_labels = train_test_split(all_texts, all_labels, test_size=0.2, random_state=42)\nall_texts = None\nall_labels = None","metadata":{"execution":{"iopub.status.busy":"2024-05-11T23:48:28.877624Z","iopub.execute_input":"2024-05-11T23:48:28.877905Z","iopub.status.idle":"2024-05-11T23:48:28.920251Z","shell.execute_reply.started":"2024-05-11T23:48:28.877864Z","shell.execute_reply":"2024-05-11T23:48:28.919493Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# Tokenize and encode the training and validation sets\ntrain_encodings = tokenizer(train_texts, truncation=True, padding=True, return_tensors='pt')\nval_encodings = tokenizer(val_texts, truncation=True, padding=True, return_tensors='pt')","metadata":{"execution":{"iopub.status.busy":"2024-05-11T23:48:30.997413Z","iopub.execute_input":"2024-05-11T23:48:30.997836Z","iopub.status.idle":"2024-05-11T23:48:33.334254Z","shell.execute_reply.started":"2024-05-11T23:48:30.997805Z","shell.execute_reply":"2024-05-11T23:48:33.333453Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# Move tensors to GPU\ntrain_encodings = {key: value.to(device) for key, value in train_encodings.items()}\nval_encodings = {key: value.to(device) for key, value in val_encodings.items()}","metadata":{"execution":{"iopub.status.busy":"2024-05-11T23:48:42.424602Z","iopub.execute_input":"2024-05-11T23:48:42.424961Z","iopub.status.idle":"2024-05-11T23:48:42.532988Z","shell.execute_reply.started":"2024-05-11T23:48:42.424934Z","shell.execute_reply":"2024-05-11T23:48:42.531944Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# Convert labels to PyTorch tensors\ntrain_labels = torch.tensor(train_labels).to(device)\nval_labels = torch.tensor(val_labels).to(device)","metadata":{"execution":{"iopub.status.busy":"2024-05-11T23:48:42.725382Z","iopub.execute_input":"2024-05-11T23:48:42.725721Z","iopub.status.idle":"2024-05-11T23:48:42.731551Z","shell.execute_reply.started":"2024-05-11T23:48:42.725694Z","shell.execute_reply":"2024-05-11T23:48:42.730433Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# Create PyTorch datasets\ntrain_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], train_labels)\nval_dataset = TensorDataset(val_encodings['input_ids'], val_encodings['attention_mask'], val_labels)","metadata":{"execution":{"iopub.status.busy":"2024-05-11T23:48:43.506613Z","iopub.execute_input":"2024-05-11T23:48:43.507335Z","iopub.status.idle":"2024-05-11T23:48:43.512046Z","shell.execute_reply.started":"2024-05-11T23:48:43.507304Z","shell.execute_reply":"2024-05-11T23:48:43.511150Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"# Create PyTorch data loaders\nbatch_size = 4\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-05-11T23:48:44.890778Z","iopub.execute_input":"2024-05-11T23:48:44.891494Z","iopub.status.idle":"2024-05-11T23:48:44.896465Z","shell.execute_reply.started":"2024-05-11T23:48:44.891462Z","shell.execute_reply":"2024-05-11T23:48:44.895399Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"new_model = fine_tune_roberta_for_rumor_detection(model, tokenizer, train_loader, val_loader, ewc=ewc, model_save_path='NFRD/nfrd-model')","metadata":{"execution":{"iopub.status.busy":"2024-05-11T23:48:46.233390Z","iopub.execute_input":"2024-05-11T23:48:46.234268Z","iopub.status.idle":"2024-05-12T00:31:37.452195Z","shell.execute_reply.started":"2024-05-11T23:48:46.234234Z","shell.execute_reply":"2024-05-12T00:31:37.450913Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/20, Train Loss: 0.00014754000585526228, Train Accuracy: 1.0000, Val Loss: 0.0228, Val Accuracy: 0.9975\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50c9f1c2477e46b59ee94a08e862fe68"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee4b24fbf8c54e46a2495987a8aee9f6"}},"metadata":{}},{"name":"stdout","text":"Model checkpoint saved to NFRD/nfrd-model\nModel uploaded to the Hugging Face Model Hub\nEpoch 2/20, Train Loss: 8.588639320805669e-05, Train Accuracy: 1.0000, Val Loss: 0.0245, Val Accuracy: 0.9975\nEpoch 3/20, Train Loss: 5.9662372223101556e-05, Train Accuracy: 1.0000, Val Loss: 0.0256, Val Accuracy: 0.9975\nEpoch 4/20, Train Loss: 3.004026075359434e-05, Train Accuracy: 1.0000, Val Loss: 0.0266, Val Accuracy: 0.9975\nEpoch 5/20, Train Loss: 2.1099811419844627e-05, Train Accuracy: 1.0000, Val Loss: 0.0274, Val Accuracy: 0.9975\nEpoch 6/20, Train Loss: 1.7255386410397477e-05, Train Accuracy: 1.0000, Val Loss: 0.0281, Val Accuracy: 0.9975\nEarly stopping after 6 epochs without improvement.\n","output_type":"stream"}]}]}
